![CCMI](ccmi_logo.png)


# CCMI â€” Customized Consecutive Machine Interpreter

> **ğŸ›ï¸ We Customized Translation, Why Not Interpreting?**

**CCMI** is a desktop PyQt5 app that turns your microphone into a *customizable consecutive interpreter*:
**mic â†’ Whisper ASR â†’ GPT (translate with brief & termlist) â†’ optional TTS.**
It adapts to the session (oneâ€‘way, twoâ€‘party, or twoâ€‘party + audience), keeps terminology consistent, and speaks back in the voice you choose.

---

## Why interpreting needs customization

- **ğŸ§µ Customization doesnâ€™t cross the mic.**
  Traditional tools treat interpreting as one-size-fits-all: no preâ€‘briefs, no termlists, no tone or audience intent.
- **â±ï¸ Too many steps, too much lag.**
  Speech â†’ text â†’ translation â†’ voice. Each hop adds delay and loses detail.
- **ğŸ§© Sessions are not identical.**
  A sales call, a lecture, and a panel get treated the same. No session modes or memory per party.

## â­ Why CCMI?
- **ğŸ§­ Fits your setup.**
  Solo talk? Twoâ€‘person call? Conversation with listeners? Pick the right mode so roles and direction are clear.
- **ğŸ™ï¸ Tell it once, CCMI prepares.**
  Describe your session and CCMI fills a minimal brief: purpose, roles, tone, and rules.
- **ğŸ“š Your terms, locked in.**
  Import a list or add your own. Names and phrases stay consistent across the whole session.
- **ğŸ§ Voices for every tone.**
  Choose among builtâ€‘in voice styles and test them anytime.
- **ğŸ§  Context that grows.**
  CCMI remembers the session: new segments adapt to your brief, prior translations, and termlist.

---

## Features
- **Session modes**: Oneâ€‘way, Twoâ€‘party, Twoâ€‘party + Audience
- **Briefs** for each direction, plus Audience fields when relevant
- **Termlist** (CSV/XLSX import, inâ€‘app edit, clear/add/delete)
- **Review table** (copy last; **XLSX export**)
- **Device picker & meters**, swap languages (Ctrl/Cmd+Enter), **Shift+Space** to record
- **Voice tester** and multiple TTS styles
- **Privacy**: API key lives in memory only; temp audio files are removed
- **Online**: uses OpenAI APIs for ASR, translation, and TTS

## How it works
1. **Record** a segment (Shift+Space)
2. **ASR**: audio â†’ text (OpenAI `whisper-1`)
3. **Translate** with GPT using:
   - current **session brief** (purpose, roles, toneâ€¦),
   - rolling **context** (recent translations),
   - enforced **terminology** (`source = target` pairs).
4. **TTS** (optional) with `gpt-4o-mini-tts` in the selected voice.

---

## Quick start

### Option A â€” Windows executable (no Python required)
1. Go to the repoâ€™s **Releases** page and download the latest `CCMI_Windows_x64.zip`.
2. Unzip and run **`ccmi.exe`**.
3. When prompted, paste your **OpenAI API key** (stored only in memory for this session).

### Option B â€” Run from source (Windows/macOS/Linux)
- Requirements: **Python 3.9+**, microphone/speaker devices.
- Install:
  ```bash
  python -m venv .venv
  # Windows
  .venv\Scripts\activate
  # macOS / Linux
  source .venv/bin/activate

  pip install -r requirements.txt
  python ccmi.py
  ```
  On first launch, click **â€œSet API Keyâ€** and enter an `sk-â€¦` key.

### Keyboard shortcuts
- **Shift+Space** â€“ Start/Stop recording
- **Ctrl+Enter** (Windows/Linux) or **âŒ˜ Return** (macOS) â€“ Swap Source â†” Target

---

## Session modes
- **ğŸ¯ Oneâ€‘Way (A â†’ B)** â€” for announcements/lectures; has Audience fields.
- **ğŸ¤ Twoâ€‘Party (A â†” B)** â€” for conversations/calls; separate briefs for each direction.
- **ğŸ¤ Twoâ€‘Party + Audience (A â†” B + ğŸ‘¥)** â€” conversations with listeners; adds Audience fields.

## Terminology (CSV/XLSX)
- Two columns: **Source Term**, **Target Term** (header names are not required).
- Import via **Termlist â†’ â€œImport Termlist (XLSX/CSV)â€**.
- During translation, exact pairs are enforced (format used in prompt: `source = target`).  
  *(Excel support requires `openpyxl` â€” optional dependency.)*

## Export & review
- **Review â†’ Export XLSX** saves the segment table with wrapped cells.
- **Copy Last** puts the newest translation on your clipboard.

---

## Build the Windows executable yourself
This repo ships with **`ccmi.spec`** for PyInstaller.

```bash
pip install -r requirements.txt
pip install pyinstaller
pyinstaller ccmi.spec
```

- The executable will be in `dist/`. Donâ€™t commit `dist/` or `build/`; publish the zip as a **Release asset**.
- Keep `ccmi.spec` tracked in git.

## Tech notes
- GUI: **PyQt5**
- Audio I/O: **sounddevice / soundfile**, 16 kHz mono capture
- ASR: **OpenAI `whisper-1`** (`client.audio.transcriptions.create`)
- Translation: **Chat Completions** (model configurable, e.g., `gpt-4.1-2025-04-14`)
- TTS: **`gpt-4o-mini-tts`** with selectable voices (`alloy`, `ash`, `ballad`, `coral`, `echo`, `sage`, `shimmer`, `verse`)

## Privacy & security
- API key is requested at runtime and **not written to disk**.
- Temporary audio files are **deleted** after use.
- No telemetry, analytics, or background upload.
- You are responsible for complying with OpenAI API Terms in your region.

---

## Troubleshooting
- **â€œNo OpenAI API key set.â€** â†’ Click **Set API Key** in the topâ€‘right.
- **â€œNo usable audio devices were found.â€** â†’ Plug in a mic/speaker and reopen CCMI.
- **Import XLSX fails** â†’ Install `openpyxl` or import a CSV instead.
- **No sound on playback** â†’ Pick an output device (ğŸ”Š badge) and test a voice with **ğŸ§ª Hear**.
- **Large prompts** â†’ CCMI caps rolling context to ~5,000 chars to keep latency reasonable.

## Contributing
PRs and issues are welcome! See **CONTRIBUTING.md** for guidance.

## License
**MIT** â€” see `LICENSE`.

<p align="center">
  <a href="../../releases/latest">
    <img alt="Download for Windows" src="https://img.shields.io/badge/Download-Windows%20ZIP-blue">
  </a>
  <a href="../../releases/latest">
    <img alt="Latest release" src="https://img.shields.io/github/v/release/pasabayramoglu/ccmi?label=latest">
  </a>
</p>

**â¡ï¸ Quick download:** Get the ready-to-use Windows build from the
**[latest release](../../releases/latest)**. Unzip and run `ccmi.exe`.
